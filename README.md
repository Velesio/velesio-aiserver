# ğŸš€ Graycat AI Server

High-performance, microservice-based AI inference server with Unity integration support.

[![Deploy on RunPod](https://img.shields.io/badge/Deploy%20on-RunPod-blue?style=for-the-badge)](https://console.runpod.io/deploy?template=3rsr5dzv50&ref=muhg2w55)

## âœ¨ Features

- **ğŸ¯ Unity Ready:** Seamless integration with Unity Assets like "LLM for Unity"
- **âš¡ High Performance:** Custom llama.cpp binary with GPU acceleration
- **ğŸ“ˆ Scalable:** Redis queue-based worker architecture
- **ğŸ³ Easy Deploy:** Docker Compose setup with auto-downloading models
- **ğŸ“Š Monitoring:** Built-in Grafana dashboards and observability

## ğŸ“š Documentation

ğŸ“– **[Complete Documentation](https://docs.graycat.ai)** - Full guides, API reference, and examples

- ğŸš€ **[Getting Started](docs/getting-started.md)** - Installation and basic setup
- ğŸ—ï¸ **[Architecture](docs/architecture.md)** - System design and components  
- ğŸš¢ **[Deployment](docs/deployment.md)** - Production deployment strategies
- ğŸ”Œ **[API Reference](docs/api-reference.md)** - Complete endpoint documentation
- ğŸ› ï¸ **[Components](docs/components.md)** - Individual service configuration
- ğŸ”§ **[Troubleshooting](docs/troubleshooting.md)** - Common issues and solutions

## âš¡ Quick Start

### RunPod Template
Deploy instantly on RunPod GPU cloud:
- ğŸ”— **[One-Click Deploy](https://console.runpod.io/deploy?template=3rsr5dzv50&ref=muhg2w55)**
- Set `REMOTE=false` for standalone inference endpoint (port 1337)
- Set `REMOTE=true` to connect to your Redis queue remotely

### Local Setup
```bash
# 0. Environment:
Linux
NVIDIA GPU with cuda 12.2+ drivers installed
NVIDIA Container Toolkit
Docker compose plugin

# 1. Get the code
git clone https://github.com/GrayCatHQ/graycat-aiserver.git
cd graycat-aiserver

# 2. Setup models and binaries
cd gpu && ./server_setup.sh && cd ..

# 3. Configure environment
cp .env.example .env  # Edit tokens and model URLs

# 4. Launch!
docker-compose up -d

# 5. If you are locally developing you can use the --build flag, and include the undreamai_server binaries in the /gpu dir
docker-compose up -d --build
```

Your API will be available at `http://localhost:8000` ğŸ‰

> ğŸ“– **Need more details?** Check out the **[Getting Started Guide](docs/getting-started.md)** for comprehensive setup instructions.

## ğŸ—ï¸ Architecture

Distributed microservice design for maximum flexibility:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    API      â”‚â”€â”€â”€â”€â”‚  Redis  â”‚â”€â”€â”€â”€â”‚ GPU Workers â”‚
â”‚  (FastAPI)  â”‚    â”‚ Queue   â”‚    â”‚ (LLM + SD)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                                  â”‚
       â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Monitoring  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚(Grafana+Prom)â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **API Service:** FastAPI with token auth and job queuing
- **GPU Workers:** Custom llama.cpp + Stable Diffusion inference engines  
- **Redis Queue:** Decoupled job processing for scalability
- **Monitoring:** Pre-configured Grafana dashboards

> ğŸ“– **Learn more:** [Architecture Documentation](docs/architecture.md)

## ğŸ”Œ API Endpoints

| Endpoint | Purpose | Documentation |
|----------|---------|---------------|
| `POST /completion` | Unity-compatible text generation | [API Reference](docs/api-reference.md#completion) |
| `POST /generate` | Advanced text generation | [API Reference](docs/api-reference.md#generate) |
| `POST /image` | Stable Diffusion image generation | [API Reference](docs/api-reference.md#image) |
| `GET /health` | Health check (no auth) | [API Reference](docs/api-reference.md#health) |

Authentication: `Authorization: Bearer your-token-here`

## ğŸ® Unity Integration

Built specifically for Unity developers:

- **[LLM for Unity](https://assetstore.unity.com/packages/tools/ai-ml-integration/llm-for-unity-273604)** - Recommended Unity asset
- Compatible `/completion` endpoint for seamless integration
- Base64 image encoding for Stable Diffusion support

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

**Questions?** Check the [Documentation](docs/) or open an issue!
