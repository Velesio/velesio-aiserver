version: "3.9"

services:
  redis:
    image: redis:7-alpine
    container_name: redis
    command: redis-server --requirepass "${REDIS_PASS}" --timeout 0
    environment:
      - REDIS_PASS=${REDIS_PASS}
    ports:
      - "6379:6379"
    restart: unless-stopped
    labels:
      container_name: "redis"
      service_name: "velesio"
      pod_name: "redis-pod"

  api:
    image: ghcr.io/velesio/velesio-api:latest
    build:
      context: ./api
      dockerfile: Dockerfile
    container_name: velesio-api
    depends_on:
      - redis
    environment:
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PASS=${REDIS_PASS}
      - API_TOKENS=${API_TOKENS}
    ports:
      - "8000:8000"
    restart: unless-stopped
    labels:
      container_name: "velesio-api"
      service_name: "velesio"
      pod_name: "api-pod"

  gpu:
    profiles: ["gpu"]
    image: ghcr.io/velesio/velesio-gpu:latest  
    build:
      context: ./gpu
      dockerfile: Dockerfile
    container_name: velesio-gpu
    ports: 
      - "1337:1337"
      - "7860:7860"
    environment:
      - GPU_LAYERS=${GPU_LAYERS}
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PASS=${REDIS_PASS}
      - LLAMA_SERVER_URL=${LLAMA_SERVER_URL}
      - API=${API}
      - MODEL_URL=${MODEL_URL}
      - PORT=${PORT}
      - RUN_SD=${RUN_SD}
      - RUN_LLAMACPP=${RUN_LLAMACPP}
      - STARTUP_COMMAND=${STARTUP_COMMAND}
      - SD_STARTUP_COMMAND=${SD_STARTUP_COMMAND}
      - RUN_OLLAMA=${RUN_OLLAMA:-false}
      - OLLAMA_SERVER_URL=${OLLAMA_SERVER_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-gemma2:2b}
      - SD_SERVER_URL=${SD_SERVER_URL:-http://localhost:7860}
    # if you mount your model directory:
    restart: unless-stopped
    volumes:
      - ./gpu/data:/app/data
    depends_on:
      - api
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    labels:
      container_name: "velesio-gpu"
      service_name: "velesio"
      pod_name: "gpu-worker-pod"

  ollama:
    profiles: ["ollama"]
    image: ollama/ollama:latest
    container_name: velesio-ollama
    environment:
      - OLLAMA_MODELS=/models
      - OLLAMA_MODEL=${OLLAMA_MODEL:-gemma2:2b}
    volumes:
      - ./ollama/models:/models
      - ./ollama/entrypoint.sh:/entrypoint.sh
    ports:
      - "11434:11434"
    restart: unless-stopped
    entrypoint: /entrypoint.sh
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://127.0.0.1:11434/api/tags"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 10s
    labels:
      container_name: "velesio-ollama"
      service_name: "velesio"
      pod_name: "ollama-pod"


  nginx:
    profiles: ["nginx"]
    image: nginx:alpine
    container_name: nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - ./nginx/logs/nginx:/var/log/nginx
    restart: unless-stopped
    depends_on:
      - api
    labels:
      container_name: "nginx"
      service_name: "reverse-proxy"
      pod_name: "nginx-pod"
